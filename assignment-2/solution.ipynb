{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3979bd3-8495-45d7-a1f5-74e74d9d3878",
   "metadata": {},
   "source": [
    "# Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec61126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc634f26-e1fc-4138-92c7-1320ef88e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2hed, rgba2rgb\n",
    "from skimage.io import imread\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b4d5f",
   "metadata": {},
   "source": [
    "# Image Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/patches_256.zip\n",
    "# !unzip /content/patches_256.zip -d /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4413c21",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ae9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"data-mining\",\n",
    "#     config={\"learning_rate\": 0.001, \"architecture\": \"CNN\", \"epochs\": 5},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3b46c-091d-43b3-9d06-4168b61c6329",
   "metadata": {},
   "source": [
    "# Utility functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666c4db",
   "metadata": {},
   "source": [
    "Define some utility functions for working with images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753bc87-a3fa-4316-bf6e-43100db34331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_id: str) -> np.array:\n",
    "    \"\"\"Reads an image from the dataset\n",
    "\n",
    "    Args:\n",
    "        image_id (str): The id of the image to be read\n",
    "\n",
    "    Returns:\n",
    "        np.array: The image as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    image_folder = Path(\"data/patches_256\")\n",
    "\n",
    "    image_path = image_folder / f\"{image_id}.png\"\n",
    "\n",
    "    rgb_image = imread(image_path)\n",
    "\n",
    "    # if the image has an alpha channel, remove it\n",
    "    if rgb_image.shape[-1] == 4:\n",
    "        rgb_image = rgb_image[:, :, :3]\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccdc51-e2d8-4320-8cc1-ae77fccc418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_hed(input_rgb_image: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Converts an RGB image to the HED color space.\n",
    "\n",
    "    Parameters:\n",
    "        input_rgb_image (np.array): The input RGB image.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The image converted to the HED color space.\n",
    "    \"\"\"\n",
    "    hed_image = rgb2hed(input_rgb_image)\n",
    "    hed_image = hed_image.astype(np.float32)\n",
    "    return hed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f6bf1-f093-48b5-9b7b-21860bd37ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intensity_avg(input_image: np.array, channel: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average intensity for a specific channel in an RGB or HED image.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (np.array): The input image (RGB or HED).\n",
    "        channel (int): The channel index for which to calculate the average intensity.\n",
    "\n",
    "    Returns:\n",
    "        float: The average intensity for the specified channel.\n",
    "    \"\"\"\n",
    "    return input_image[:, :, channel].flatten().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f92ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intensity_std(input_image: np.array, channel: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of the intensity for a specific channel in an RGB or HED image.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (np.array): The input image (RGB or HED).\n",
    "        channel (int): The channel index for which to calculate the standard deviation of the intensity.\n",
    "\n",
    "    Returns:\n",
    "        float: The standard deviation of the intensity for the specified channel.\n",
    "    \"\"\"\n",
    "    return input_image[:, :, channel].flatten().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775192a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_h_intensity(image_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the average H intensity of an image.\n",
    "\n",
    "    Parameters:\n",
    "        image_id (str): The ID of the image.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the image ID and the average H intensity.\n",
    "    \"\"\"\n",
    "    rgb_image = read_image(image_id)\n",
    "    hed_image = convert_rgb_to_hed(rgb_image)\n",
    "    avg_h_intensity = calculate_intensity_avg(hed_image, 0)\n",
    "    \n",
    "    del rgb_image, hed_image\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"avg_h_intensity\": avg_h_intensity,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(\n",
    "        \"https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/protein_expression_data.csv\"\n",
    "    )\n",
    "\n",
    "    # create specimen id field\n",
    "    df[\"specimen_id\"] = df.VisSpot.apply(lambda x: x.split(\"-\")[2])\n",
    "\n",
    "    # create image id field\n",
    "    df[\"image_id\"] = df.VisSpot.apply(lambda x: x.split(\"-\")[2]) + \"_\" + df.id\n",
    "\n",
    "    df = df.set_index(\"image_id\").sort_index()\n",
    "\n",
    "    training_specimens = [\"A1\", \"B1\", \"D1\"]\n",
    "    testing_specimens = [\"C1\"]\n",
    "\n",
    "    training_data = df.loc[df[\"specimen_id\"].isin(training_specimens)]\n",
    "    testing_data = df.loc[df[\"specimen_id\"].isin(testing_specimens)]\n",
    "\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    return {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"pearson_corr\": pearson_corr,\n",
    "        \"spearman_corr\": spearman_corr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4feee-fe35-4e2f-a645-2016dc327593",
   "metadata": {},
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67a3f4",
   "metadata": {},
   "source": [
    "Load the protein expression data after splitting it into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde263ed-d1e2-44a2-acec-be5ea0acd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(training_data)}\")\n",
    "print(f\"Number of testing samples: {len(testing_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde55c9-b8a0-4222-a6ea-0a6c00906314",
   "metadata": {},
   "source": [
    "# Question No. 1: (Data Analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52714c1a",
   "metadata": {},
   "source": [
    "For the following questions, we will use only the `training_data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712ad53-91b4-4599-a3ea-cc0a866bb3e0",
   "metadata": {},
   "source": [
    "## Counting Examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a7aa6-a359-4782-a815-00ad637089e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    training_data.groupby(\"specimen_id\", as_index=False)\n",
    "    .agg(n_sample=(\"id\", \"count\"))\n",
    "    .sort_values(\"n_sample\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ceef8",
   "metadata": {},
   "source": [
    "We see that specimen `A1` has the largest numer of spots (almost double)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a11daf-7d93-4d81-89b3-8aeefbb8ac99",
   "metadata": {},
   "source": [
    "## Protein Expression Histograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59657508",
   "metadata": {},
   "outputs": [],
   "source": [
    "protiens = [\"NESTIN\", \"cMYC\", \"MET\"]\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot protein histograms for specimen A1\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, protein in enumerate(protiens):\n",
    "\n",
    "    sns.histplot(\n",
    "        training_data.query(\"specimen_id == 'A1'\"), x=protein, ax=ax[i], color=colors[i]\n",
    "    )\n",
    "\n",
    "    mean = training_data.query(\"specimen_id == 'A1'\")[protein].mean()\n",
    "    std = training_data.query(\"specimen_id == 'A1'\")[protein].std()\n",
    "    skew = training_data.query(\"specimen_id == 'A1'\")[protein].skew()\n",
    "    min = training_data.query(\"specimen_id == 'A1'\")[protein].min()\n",
    "    max = training_data.query(\"specimen_id == 'A1'\")[protein].max()\n",
    "\n",
    "    # decrease font size in set_title\n",
    "    ax[i].set_title(\n",
    "        f\"Specimen A1\\nmean: {mean:.2f}\\nstd: {std:.2f}\\nskew: {skew:.2f}\\nmin: {min:.2f}, max: {max:.2f}\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax[i].set_xlabel(protein)\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot protein histograms for specimen B1\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, protein in enumerate(protiens):\n",
    "\n",
    "    sns.histplot(\n",
    "        training_data.query(\"specimen_id == 'B1'\"), x=protein, ax=ax[i], color=colors[i]\n",
    "    )\n",
    "\n",
    "    mean = training_data.query(\"specimen_id == 'B1'\")[protein].mean()\n",
    "    std = training_data.query(\"specimen_id == 'B1'\")[protein].std()\n",
    "    skew = training_data.query(\"specimen_id == 'B1'\")[protein].skew()\n",
    "    min = training_data.query(\"specimen_id == 'B1'\")[protein].min()\n",
    "    max = training_data.query(\"specimen_id == 'B1'\")[protein].max()\n",
    "\n",
    "    \n",
    "    ax[i].set_title(\n",
    "        f\"Specimen B1\\nmean: {mean:.2f}\\nstd: {std:.2f}\\nskew: {skew:.2f}\\nmin: {min:.2f}, max: {max:.2f}\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax[i].set_xlabel(protein)\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238ecfd-5caf-4858-bd17-e4bc04e69c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot protein histograms for specimen D1\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, protein in enumerate(protiens):\n",
    "\n",
    "    sns.histplot(\n",
    "        training_data.query(\"specimen_id == 'D1'\"), x=protein, ax=ax[i], color=colors[i]\n",
    "    )\n",
    "\n",
    "    mean = training_data.query(\"specimen_id == 'D1'\")[protein].mean()\n",
    "    std = training_data.query(\"specimen_id == 'D1'\")[protein].std()\n",
    "    skew = training_data.query(\"specimen_id == 'D1'\")[protein].skew()\n",
    "    min = training_data.query(\"specimen_id == 'D1'\")[protein].min()\n",
    "    max = training_data.query(\"specimen_id == 'D1'\")[protein].max()\n",
    "\n",
    "    ax[i].set_title(\n",
    "        f\"Specimen D1\\nmean: {mean:.2f}\\nstd: {std:.2f}\\nskew: {skew:.2f}\\nmin: {min:.2f}, max: {max:.2f}\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax[i].set_xlabel(protein)\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519c0bf",
   "metadata": {},
   "source": [
    "From the above plots, we notice the following:\n",
    "\n",
    "1. All histograms have very similar distribution to the normal distribution. There's an apparent left *skewnewss* in the histograms. This suggest that the protein expression values, across the specimens, have some unfrequent negative values.\n",
    "\n",
    "2. Different protients have different ranges. `NESTIN` have a slightly smaller range the `cMYC` and `MET`. The ranges are consistnet across the different specimens. However, the `NESTIN` range for `D1` is thinner and has smaller negative values. The high negative values in `NESTIN` which are present in `A1` and `B1` but absent in `D1` might affect the learning algorithm to learn the patterns.\n",
    "\n",
    "3. Spread and central tendency: overall, the different protein across the different specimens have mean close to 0 and standard deviation close to 1.\n",
    "\n",
    "4. Outliers: the extreme negative points can be though as outliers and might be challenging for the model to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec311e42-1f84-4bf1-a285-ba7560aa1f25",
   "metadata": {},
   "source": [
    "## Image Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1185b6",
   "metadata": {},
   "source": [
    "To have a good understanding of the potential usefulness of the HED color space, we'll select examples from the training data where NESTIN has very low, average, and very high values.\n",
    "\n",
    "This will hopefully allow us see if we can use the HED color space as more effective reprsentation of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ec182",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nestin_a = training_data.loc[training_data[\"specimen_id\"] == \"A1\", \"NESTIN\"].mean()\n",
    "mean_nestin_b = training_data.loc[training_data[\"specimen_id\"] == \"B1\", \"NESTIN\"].mean()\n",
    "mean_nestin_d = training_data.loc[training_data[\"specimen_id\"] == \"D1\", \"NESTIN\"].mean()\n",
    "\n",
    "# select images from specimen A1\n",
    "sample_images_A1 = pd.concat(\n",
    "    [\n",
    "        # select images with the smallest NESTIN values\n",
    "        training_data.query(\"specimen_id == 'A1'\")\n",
    "        .nsmallest(3, \"NESTIN\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"image_id\", \"NESTIN\"]],\n",
    "        # select images with NESTIN values around the mean\n",
    "        training_data.loc[\n",
    "            (training_data[\"specimen_id\"] == \"A1\")\n",
    "            & (training_data[\"NESTIN\"] > mean_nestin_a - 0.1)\n",
    "            & (training_data[\"NESTIN\"] < mean_nestin_a + 0.1),\n",
    "            \"NESTIN\",\n",
    "        ]\n",
    "        .head(3)\n",
    "        .reset_index(),\n",
    "        # select images with the largest NESTIN values\n",
    "        training_data.query(\"specimen_id == 'A1'\")\n",
    "        .nlargest(3, \"NESTIN\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"image_id\", \"NESTIN\"]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select images from specimen B1\n",
    "sample_images_B1 = pd.concat(\n",
    "    [\n",
    "        # select images with the smallest NESTIN values\n",
    "        training_data.query(\"specimen_id == 'B1'\")\n",
    "        .nsmallest(3, \"NESTIN\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"image_id\", \"NESTIN\"]],\n",
    "        # select images with NESTIN values around the mean\n",
    "        training_data.loc[\n",
    "            (training_data[\"specimen_id\"] == \"B1\")\n",
    "            & (training_data[\"NESTIN\"] > mean_nestin_b - 0.1)\n",
    "            & (training_data[\"NESTIN\"] < mean_nestin_b + 0.1),\n",
    "            \"NESTIN\",\n",
    "        ]\n",
    "        .head(3)\n",
    "        .reset_index(),\n",
    "        # select images with the largest NESTIN values\n",
    "        training_data.query(\"specimen_id == 'B1'\")\n",
    "        .nlargest(3, \"NESTIN\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"image_id\", \"NESTIN\"]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select images from specimen D1\n",
    "sample_images_D1 = pd.concat(\n",
    "    [\n",
    "        # select images with the smallest NESTIN values\n",
    "        training_data.query(\"specimen_id == 'D1'\")\n",
    "        .nsmallest(3, \"NESTIN\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"image_id\", \"NESTIN\"]],\n",
    "        # select images with NESTIN values around the mean\n",
    "        training_data.loc[\n",
    "            (training_data[\"specimen_id\"] == \"D1\")\n",
    "            & (training_data[\"NESTIN\"] > mean_nestin_d - 0.1)\n",
    "            & (training_data[\"NESTIN\"] < mean_nestin_d + 0.1),\n",
    "            \"NESTIN\",\n",
    "        ]\n",
    "        .head(3)\n",
    "        .reset_index(),\n",
    "        # select images with the largest NESTIN values\n",
    "        training_data.query(\"specimen_id == 'D1'\")\n",
    "        .nlargest(3, \"NESTIN\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"image_id\", \"NESTIN\"]],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_data = (\n",
    "    pd.concat([sample_images_A1, sample_images_B1, sample_images_D1])\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(\"NESTIN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d99d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {len(all_sample_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c28fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in all_sample_data.iterrows():\n",
    "    image_id = row[\"image_id\"]\n",
    "    nestin_value = row[\"NESTIN\"]\n",
    "\n",
    "    img = read_image(image_id)\n",
    "    hed_img = convert_rgb_to_hed(img)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(f\"Image ID: {image_id} NESTIN = {nestin_value:.2f}\")\n",
    "\n",
    "    ax[1].imshow(hed_img[:, :, 0], cmap=\"gray\")\n",
    "    ax[1].set_title(\"Hematoxylin\")\n",
    "\n",
    "    ax[2].imshow(hed_img[:, :, 1], cmap=\"gray\")\n",
    "    ax[2].set_title(\"Eosin\")\n",
    "\n",
    "    ax[3].imshow(hed_img[:, :, 2], cmap=\"gray\")\n",
    "    ax[3].set_title(\"DAB\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500a667",
   "metadata": {},
   "source": [
    "Without an expert domain knowledge in the working of tissue images and the measurement of different protiens, it will be hard to accurtly describe what this conversion is doing and why certain images have low values for NESTIN, while others have higher values.\n",
    "\n",
    "However, we can quickly see that HED color space is a good representation of the image. In particular, the H and D channels.\n",
    "\n",
    "The H channel highlights the cellular nuclei while the D channel seems to detect the *pink* areas.\n",
    "\n",
    "In most cases, the E channel seems unresponsive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa5676-4b8c-4d08-9c1c-04c6c4c4c07c",
   "metadata": {},
   "source": [
    "## H-channel Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_h_intensity_list = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(calculate_avg_h_intensity)(image_id) for image_id in training_data.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdfb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_h_intensity_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0513f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_h_intensity_df = pd.DataFrame(avg_h_intensity_list).set_index(\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd478ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NESTIN and specimen_id to the dataframe\n",
    "avg_h_intensity_df = avg_h_intensity_df.join(training_data[[\"NESTIN\", \"specimen_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_h_intensity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = avg_h_intensity_df[\"avg_h_intensity\"].corr(avg_h_intensity_df[\"NESTIN\"])\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    data=avg_h_intensity_df,\n",
    "    x=\"avg_h_intensity\",\n",
    "    y=\"NESTIN\",\n",
    "    hue=\"specimen_id\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "ax.set_title(\"Average H intensity vs NESTIN expression\\nCorrelation: {:.2f}\".format(correlation))\n",
    "ax.set_xlabel(\"Average H intensity\")\n",
    "ax.set_ylabel(\"NESTIN expression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee15134",
   "metadata": {},
   "source": [
    "From the scatter plot and the correlation value we can see that there is a positive relation between the average intensity value of the `H` channel and the expression levels of `NESTIN`.\n",
    "\n",
    "However, this correlation is weak and won't capture the true relation of the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0053f17",
   "metadata": {},
   "source": [
    "## Performance Metrics for Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7642140",
   "metadata": {},
   "source": [
    "Our problem is a regression problem where we want to predict a continous value and compare it with the target variable.\n",
    "\n",
    "The target variable (`NESTTIN` for example) takes on values between -6 and 2.\n",
    "\n",
    "Some of the common regression performance metrics:\n",
    "\n",
    "1. **Mean Absolute Error**: this metric takes the absolute difference between predicted values and ture values. Lower values (closer to 0) means better prediction.\n",
    "\n",
    "2. **Mean Squared Error**: this metric takes the squred difference between predictive values and true values. Since it's _squared_, it's a polynomial function. One disadvantage of this function is the difficulty in interpreting its values because it squares the values.\n",
    "\n",
    "3. **Root Mean Squared Error**: to address the issue of interpreting the results in MSE, this metric takes the squared root to the MSE value, resulting in a value in the same unit as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b633c0",
   "metadata": {},
   "source": [
    "# Question No. 2: (Feature Extraction and Classical Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd5693",
   "metadata": {},
   "source": [
    "## Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e236281",
   "metadata": {},
   "source": [
    "The purpose of this section is to extract candidate features _f<sub>1</sub>_, _f<sub>2</sub>_, _f<sub>3</sub>_, ..., _f<sub>n</sub>_ from the original images, and measure the correlation between these features and the target varible.\n",
    "\n",
    "Then, use the candidate features to perform regression using traditional machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ecfec",
   "metadata": {},
   "source": [
    "Since the work on this and the following questions will require experimenting with different appraoches and judging their performance on a hold-out data and in the spirit of avoiding any leakge from the test data, we split our original training data into **training** and **validation**.\n",
    "\n",
    "For **training** we use specimens `A1` and `D1` and for **validation** we use specimen `B1`.\n",
    "\n",
    "This way, we ensure that the final **testing** data of specimen `C1` will not influence the feature engineering or model development and will be used only at the very last to measure the performance of different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation sets\n",
    "validation_data = training_data[training_data[\"specimen_id\"].isin([\"B1\"])]\n",
    "training_data = training_data[training_data[\"specimen_id\"].isin([\"A1\", \"D1\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(training_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of testing samples: {len(testing_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ce618",
   "metadata": {},
   "source": [
    "Before we dive in, we resize our original images from `256`x`256` to `64`x`64`.\n",
    "\n",
    "While there will be a loss of information, but this is essential for computational feasibility.\n",
    "\n",
    "Let's first resize a single image and see how it compares to the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = read_image(\"A1_0x40\")\n",
    "resized_image = skimage.transform.resize(original_image, (64, 64), anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original image shape: {original_image.shape}\")\n",
    "print(f\"Resized image shape: {resized_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43807e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(original_image)\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(resized_image)\n",
    "ax[1].set_title(\"Resized image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb73c7e",
   "metadata": {},
   "source": [
    "We quickly see that the image has lost some of its quality, but the spatial information are still recognizable and can be fed into a machine learning algorithm.\n",
    "\n",
    "Let's see the change in the actual values of the pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original image: (min, max) = ({original_image.min()}, {original_image.max()})\")\n",
    "print(f\"Resized image: (min, max) = ({resized_image.min()}, {resized_image.max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554efa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].hist(original_image.flatten(), bins=100, color=\"blue\", alpha=0.7)\n",
    "ax[0].set_title(\"Original image\")\n",
    "\n",
    "ax[1].hist(resized_image.flatten(), bins=100, color=\"red\", alpha=0.7)\n",
    "ax[1].set_title(\"Resized image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a946345",
   "metadata": {},
   "source": [
    "After resizing, the image was normalized to the range `[0-1]` but it persevered the same distribution. Therefore, the image can be use for further tasks such as PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a4d45",
   "metadata": {},
   "source": [
    "### Channel statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9986ea9-d173-4e90-9eb3-587e2d164982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_channel_stats(image_id: str, resize=False):\n",
    "    \"\"\"\n",
    "    Calculate the intensity statistics for each channel of an image.\n",
    "\n",
    "    Args:\n",
    "        image_id (str): The ID of the image.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the image ID and the calculated intensity statistics for each channel.\n",
    "            - \"image_id\": The ID of the image.\n",
    "            - \"h_intensity_avg\": The average intensity of the H channel in the HED color space.\n",
    "            - \"h_intensity_std\": The standard deviation of the intensity of the H channel in the HED color space.\n",
    "            - \"r_intensity_avg\": The average intensity of the R channel in the RGB color space.\n",
    "            - \"r_intensity_std\": The standard deviation of the intensity of the R channel in the RGB color space.\n",
    "            - \"g_intensity_avg\": The average intensity of the G channel in the RGB color space.\n",
    "            - \"g_intensity_std\": The standard deviation of the intensity of the G channel in the RGB color space.\n",
    "            - \"b_intensity_avg\": The average intensity of the B channel in the RGB color space.\n",
    "            - \"b_intensity_std\": The standard deviation of the intensity of the B channel in the RGB color space.\n",
    "    \"\"\"\n",
    "    rgb_image = read_image(image_id)\n",
    "\n",
    "    if resize:\n",
    "        rgb_image = skimage.transform.resize(rgb_image, (64, 64), anti_aliasing=True)\n",
    "\n",
    "    hed_image = convert_rgb_to_hed(rgb_image)\n",
    "\n",
    "    h_intensity_avg = calculate_intensity_avg(hed_image, 0)\n",
    "    h_intensity_std = calculate_intensity_std(hed_image, 0)\n",
    "\n",
    "    r_intensity_avg = calculate_intensity_avg(rgb_image, 0)\n",
    "    r_intensity_std = calculate_intensity_std(rgb_image, 0)\n",
    "\n",
    "    g_intensity_avg = calculate_intensity_avg(rgb_image, 1)\n",
    "    g_intensity_std = calculate_intensity_std(rgb_image, 1)\n",
    "\n",
    "    b_intensity_avg = calculate_intensity_avg(rgb_image, 2)\n",
    "    b_intensity_std = calculate_intensity_std(rgb_image, 2)\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"h_intensity_avg\": h_intensity_avg,\n",
    "        \"h_intensity_std\": h_intensity_std,\n",
    "        \"r_intensity_avg\": r_intensity_avg,\n",
    "        \"r_intensity_std\": r_intensity_std,\n",
    "        \"g_intensity_avg\": g_intensity_avg,\n",
    "        \"g_intensity_std\": g_intensity_std,\n",
    "        \"b_intensity_avg\": b_intensity_avg,\n",
    "        \"b_intensity_std\": b_intensity_std,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210701eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_channels_stats_list = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(calculate_image_channel_stats)(image_id, True)\n",
    "    for image_id in training_data.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_channels_stats_df = pd.DataFrame(\n",
    "    training_images_channels_stats_list\n",
    ").set_index(\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_channels_stats_df = training_images_channels_stats_df.join(\n",
    "    training_data[[\"NESTIN\", \"specimen_id\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_channels_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5ab85",
   "metadata": {},
   "source": [
    "The above dataframe contains the image channel statistics (meand and standard deviation) for each image in the training data (specimens `A1` and `D1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c67ad",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images_np_dataset(input_df):\n",
    "    rgb_images_list = []\n",
    "    hed_images_list = []\n",
    "\n",
    "    for image_id in tqdm(input_df.index):\n",
    "        rgb_image = read_image(image_id)\n",
    "        resized_image = skimage.transform.resize(\n",
    "            rgb_image, (64, 64), anti_aliasing=True\n",
    "        )\n",
    "        hed_image = convert_rgb_to_hed(resized_image)\n",
    "\n",
    "        rgb_images_list.append(resized_image)\n",
    "        hed_images_list.append(hed_image)\n",
    "\n",
    "    rgb_images = np.array(rgb_images_list)\n",
    "    hed_images = np.array(hed_images_list)\n",
    "\n",
    "    del rgb_images_list, hed_images_list\n",
    "\n",
    "    return rgb_images, hed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rgb_images, train_hed_images = prepare_images_np_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rgb_images, val_hed_images = prepare_images_np_dataset(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rgb_images.shape, train_hed_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rgb_images.shape, val_hed_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#   investigate the correct usage of gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_rgb = PCA(svd_solver=\"randomized\", random_state=42)\n",
    "pca_hed = PCA(svd_solver=\"randomized\", random_state=42)\n",
    "pca_hd = PCA(svd_solver=\"randomized\", random_state=42)\n",
    "pca_h = PCA(svd_solver=\"randomized\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = train_rgb_images.shape[0]\n",
    "\n",
    "X_train_rgb = train_rgb_images.reshape(n_train_samples, -1)\n",
    "X_train_hed = train_hed_images.reshape(n_train_samples, -1)\n",
    "\n",
    "# H and D channels\n",
    "X_train_hd = train_hed_images[:, :, :, [0, 2]].reshape(n_train_samples, -1)\n",
    "\n",
    "# only H channel\n",
    "X_train_h = train_hed_images[:, :, :, 0].reshape(n_train_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede436c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val_samples = val_rgb_images.shape[0]\n",
    "\n",
    "X_val_rgb = val_rgb_images.reshape(n_val_samples, -1)\n",
    "X_val_hed = val_hed_images.reshape(n_val_samples, -1)\n",
    "\n",
    "# H and D channels\n",
    "X_val_hd = val_hed_images[:, :, :, [0, 2]].reshape(n_val_samples, -1)\n",
    "\n",
    "# only H channel\n",
    "X_val_h = val_hed_images[:, :, :, 0].reshape(n_val_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train_rgb shape: {X_train_rgb.shape}\")\n",
    "print(f\"X_train_hed shape: {X_train_hed.shape}\")\n",
    "print(f\"X_train_hd shape: {X_train_hd.shape}\")\n",
    "print(f\"X_train_h shape: {X_train_h.shape}\")\n",
    "\n",
    "print(f\"X_val_rgb shape: {X_val_rgb.shape}\")\n",
    "print(f\"X_val_hed shape: {X_val_hed.shape}\")\n",
    "print(f\"X_val_hd shape: {X_val_hd.shape}\")\n",
    "print(f\"X_val_h shape: {X_val_h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a480567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes one minute and 28 seconds\n",
    "pca_rgb.fit(X_train_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 51 seconds\n",
    "pca_hed.fit(X_train_hed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41dee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes 42 seconds\n",
    "pca_hd.fit(X_train_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926dcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes 25 seconds\n",
    "pca_h.fit(X_train_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of components that explain 95% of the variance\n",
    "\n",
    "n_components_rgb = np.argmax(np.cumsum(pca_rgb.explained_variance_ratio_) >= 0.95) + 1\n",
    "n_components_hed = np.argmax(np.cumsum(pca_hed.explained_variance_ratio_) >= 0.95) + 1\n",
    "n_components_hd = np.argmax(np.cumsum(pca_hd.explained_variance_ratio_) >= 0.95) + 1\n",
    "n_components_h = np.argmax(np.cumsum(pca_h.explained_variance_ratio_) >= 0.95) + 1\n",
    "\n",
    "# plot a vertical line where 95% of the variance is explained\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(np.cumsum(pca_rgb.explained_variance_ratio_))\n",
    "ax[0].axvline(n_components_rgb, color=\"red\", linestyle=\"--\")\n",
    "ax[0].set_title(\n",
    "    f\"Explained variance by PCA (RGB)\\n{n_components_rgb} components\", fontsize=10\n",
    ")\n",
    "ax[0].set_xlabel(\"Number of components\")\n",
    "ax[0].set_ylabel(\"Cumulative explained variance\")\n",
    "\n",
    "ax[1].plot(np.cumsum(pca_hed.explained_variance_ratio_))\n",
    "ax[1].axvline(n_components_hed, color=\"red\", linestyle=\"--\")\n",
    "ax[1].set_title(\n",
    "    f\"Explained variance by PCA (HED)\\n{n_components_hed} components\", fontsize=10\n",
    ")\n",
    "ax[1].set_xlabel(\"Number of components\")\n",
    "\n",
    "ax[2].plot(np.cumsum(pca_hd.explained_variance_ratio_))\n",
    "ax[2].axvline(n_components_hd, color=\"red\", linestyle=\"--\")\n",
    "ax[2].set_title(\n",
    "    f\"Explained variance by PCA (H and D channels)\\n{n_components_hd} components\",\n",
    "    fontsize=10,\n",
    ")\n",
    "ax[2].set_xlabel(\"Number of components\")\n",
    "\n",
    "\n",
    "ax[3].plot(np.cumsum(pca_h.explained_variance_ratio_))\n",
    "ax[3].axvline(n_components_h, color=\"red\", linestyle=\"--\")\n",
    "ax[3].set_title(\n",
    "    f\"Explained variance by PCA (H channel)\\n{n_components_h} components\", fontsize=10\n",
    ")\n",
    "ax[3].set_xlabel(\"Number of components\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031095f",
   "metadata": {},
   "source": [
    "### GLCM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e830f5",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c72eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbac88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f4b893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fa0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7aea362",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20403486",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR_model = SVR(kernel=\"rbf\", C=1, gamma=0.1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 1227\n",
    "X_train_h_pca = pca_h.transform(X_train_h)[:,:n_comp]\n",
    "X_val_h_pca = pca_h.transform(X_val_h)[:,:n_comp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ed1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR_model.fit(X_train_h_pca, training_data[\"NESTIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = SVR_model.predict(X_val_h_pca)\n",
    "y_val_true = validation_data[\"NESTIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred.shape, y_val_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_evaluation_metrics(y_val_true, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab19c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dff9780",
   "metadata": {},
   "source": [
    "# Question No. 3 (Using Convolutional Neural Networks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d27b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # first convolutional layer\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # second convolutional layer\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(32 * 17 * 17, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer_1(x)\n",
    "        out = self.conv_layer_2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161269b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of parameters in the model\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in the model: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "training_image_ids = training_data.index.to_numpy()\n",
    "training_labels = training_data[\"NESTIN\"].to_numpy()\n",
    "\n",
    "# validation data\n",
    "validation_image_ids = validation_data.index.to_numpy()\n",
    "validation_labels = validation_data[\"NESTIN\"].to_numpy()\n",
    "\n",
    "# testing data\n",
    "testing_image_ids = testing_data.index.to_numpy()\n",
    "testing_labels = testing_data[\"NESTIN\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b182aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBToHEDTransform:\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Convert a PIL Image or numpy.ndarray from RGB to HED color space.\n",
    "\n",
    "        Parameters:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to numpy array\n",
    "        if isinstance(pic, Image.Image):\n",
    "            img_array = np.array(pic)\n",
    "        elif isinstance(pic, np.ndarray):\n",
    "            img_array = pic\n",
    "        elif torch.is_tensor(pic):\n",
    "            img_array = pic.numpy()\n",
    "            img_array = img_array.swapaxes(0, 2)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"img should be PIL Image or ndarray. Got {}\".format(type(pic))\n",
    "            )\n",
    "\n",
    "        # Convert RGB to HED. The output array from rgb2hed can have negative values,\n",
    "        # so it's important to scale and shift the values to bring them into a suitable range (e.g., 0 to 1) if necessary.\n",
    "        hed_img = rgb2hed(img_array)\n",
    "\n",
    "        # return the H channel\n",
    "        return F.to_tensor(hed_img[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_ids: np.array, labels: np.array, transform=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # read the image\n",
    "        rgb_image = read_image(image_id)\n",
    "\n",
    "        # apply transformation\n",
    "        if self.transform:\n",
    "            transformed_image = self.transform(rgb_image)\n",
    "\n",
    "        return transformed_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        RGBToHEDTransform(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CustomDataset(training_image_ids, training_labels, transformations)\n",
    "validation_dataset = CustomDataset(\n",
    "    validation_image_ids, validation_labels, transformations\n",
    ")\n",
    "testing_dataset = CustomDataset(testing_image_ids, testing_labels, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650059a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "test_dataloader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: DataLoader,\n",
    "    model: ConvNet,\n",
    "    loss_fn: nn.MSELoss,\n",
    "    optimizer: torch.optim.SGD,\n",
    "    epoch: int,\n",
    "):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # wandb.log({\"loss\": loss})\n",
    "\n",
    "    avg_train_loss = train_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602013e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    avg_val_loss = test_loss / size\n",
    "    # wandb.log({\"val_loss\": avg_val_loss})\n",
    "\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4da721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters in the model: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f94a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#   add code here to track history of the training and validation loss\n",
    "#   and then plot the loss over the epochs\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    train(train_dataloader, model, loss_fn, optimizer, epoch=t + 1)\n",
    "    test(validation_dataloader, model, loss_fn, epoch=t + 1)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45016483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
